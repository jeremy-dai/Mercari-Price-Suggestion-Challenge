{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mercari Price Suggestion Challenge\n",
    "- Task: Build an algorithm that automatically suggests the right product prices. \n",
    "- Data: User-inputted text descriptions of their products, including details like product category name, brand name, and item condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import xgboost\n",
    "from scipy import sparse\n",
    "import xgboost\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Data\n",
      "Data Imported. Time elapsed: 20s\n"
     ]
    }
   ],
   "source": [
    "print ('Importing Data')\n",
    "current_t = time.time()\n",
    "train_data = pd.read_table('data/train.tsv')\n",
    "test_data = pd.read_table('data/test.tsv')\n",
    "print(\"Data Imported. Time elapsed: \" + str(int(time.time()-current_t )) + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting features and labels\n",
      "Train/test data combined. Time elapsed: 105s\n"
     ]
    }
   ],
   "source": [
    "print ('Getting features and labels')\n",
    "current_t = time.time()\n",
    "def get_feature_label(data):\n",
    "    # split features and labels\n",
    "    train_features = data.drop(['price'],axis=1)\n",
    "    ### log transform\n",
    "    train_labels =  data.price\n",
    "    train_labels[train_labels==0]=0.01\n",
    "    train_labels = np.log(train_labels)\n",
    "    return train_features,train_labels\n",
    "train_features,train_labels=get_feature_label(train_data)\n",
    "nrow_train = train_features.shape[0]\n",
    "tt_combine = pd.concat([train_features,test_data],axis = 0)\n",
    "print(\"Train/test data combined. Time elapsed: \" + str(int(time.time()-current_t )) + \"s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting categorical var to numeric\n",
      "Handling missing data\n",
      "Coding category data\n",
      "brand_name is transformed to brand_name_le\n",
      "main_cat is transformed to main_cat_le\n",
      "subcat1 is transformed to subcat1_le\n",
      "subcat2 is transformed to subcat2_le\n",
      "subcat3 is transformed to subcat3_le\n",
      "subcat4 is transformed to subcat4_le\n",
      "Getting Length of item discription\n",
      "Creating numeric Features\n",
      "Dimension of numeric_features(2175894, 4)\n",
      "Categorical data transformed. Time elapsed: 33s\n"
     ]
    }
   ],
   "source": [
    "print ('Converting categorical var to numeric')\n",
    "current_t = time.time()\n",
    "def category(data):\n",
    "    cat = data.category_name.str.split('/', expand = True)\n",
    "    data[\"main_cat\"] = cat[0]\n",
    "    data[\"subcat1\"] = cat[1]\n",
    "    data[\"subcat2\"] = cat[2]\n",
    "    try:\n",
    "        data[\"subcat3\"] = cat[3]\n",
    "    except:\n",
    "        data[\"subcat3\"] = np.nan  \n",
    "    try:\n",
    "        data[\"subcat4\"] = cat[4]\n",
    "    except:\n",
    "        data[\"subcat4\"] = np.nan  \n",
    "category(tt_combine)\n",
    "\n",
    "print ('Handling missing data')   \n",
    "current_t = time.time()\n",
    "def missing_data(data, _value = 'None'):\n",
    "    # Handle missing data\n",
    "    for col in data.columns:\n",
    "        data[col].fillna(_value,inplace=True)\n",
    "missing_data(tt_combine)\n",
    "\n",
    "print(\"Coding category data\")\n",
    "le = preprocessing.LabelEncoder()\n",
    "def cat_to_num(data):\n",
    "    suf=\"_le\"\n",
    "    for col in ['brand_name','main_cat','subcat1','subcat2','subcat3','subcat4']:\n",
    "        data[col+suf] = le.fit_transform(data[col])\n",
    "        print(\"{} is transformed to {}\".format(col,col+suf))\n",
    "cat_to_num(tt_combine)\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "cat = enc.fit_transform(tt_combine[['main_cat_le','subcat1_le','subcat2_le','subcat3_le','subcat4_le']])\n",
    "\n",
    "\n",
    "print ('Getting Length of item discription')\n",
    "tt_combine['Length_of_item_description']=tt_combine['item_description'].apply(len)\n",
    "\n",
    "print (\"Creating numeric Features\")\n",
    "def numeric_to_features(data):\n",
    "    numeric_features = data[['shipping','item_condition_id','Length_of_item_description','brand_name_le']]\n",
    "    return numeric_features\n",
    "numeric_features = numeric_to_features(tt_combine)\n",
    "print ('Dimension of numeric_features'+str(numeric_features.shape))\n",
    "print(\"Categorical data transformed. Time elapsed: \" + str(int(time.time()-current_t )) + \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining Text\n",
      "Text data combined. Time elapsed: 64s\n",
      "Tfidf\n",
      "Dimension of text_features(2175894, 5000)\n",
      "Tfidf completed. Time elapsed: 1853s\n"
     ]
    }
   ],
   "source": [
    "print (\"Combining Text\")\n",
    "current_t = time.time()\n",
    "def text_process(data):\n",
    "    # Process text    \n",
    "    # make item_description and name lower case    \n",
    "    text = list(data.apply(lambda x:'%s %s' %(x['item_description'],x['name']), axis=1))\n",
    "    return text\n",
    "text =text_process(tt_combine)\n",
    "print(\"Text data combined. Time elapsed: \" + str(int(time.time()-current_t )) + \"s\")\n",
    "\n",
    "\n",
    "print ('Tfidf')\n",
    "current_t = time.time()\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3), stop_words = 'english',max_features = 5000)\n",
    "text_features = tfidf.fit_transform(text)\n",
    "print ('Dimension of text_features'+str(text_features.shape))\n",
    "print(\"Tfidf completed. Time elapsed: \" + str(int(time.time()-current_t )) + \"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking features\n",
      "Dimension of final_features(2175894, 6022)\n",
      "Data Ready. Time elapsed: 1886s\n"
     ]
    }
   ],
   "source": [
    "print (\"Stacking features\")\n",
    "#  Stacker for sparse data\n",
    "final_features = sparse.hstack((numeric_features,text_features,cat)).tocsr()\n",
    "print ('Dimension of final_features'+str(final_features.shape))\n",
    "train_final_features = final_features[:nrow_train]\n",
    "test_final_features = final_features[nrow_train:]\n",
    "print(\"Data Ready. Time elapsed: \" + str(int(time.time()-current_t )) + \"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the features\n",
    "pickle.dump(train_final_features,open('train_features.pkl', \"bw\"))\n",
    "pickle.dump(test_final_features,open('test_features.pkl', \"bw\"))\n",
    "pickle.dump(train_labels,open('train_labels.pkl', \"bw\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick and Tune the Algorithms\n",
    "\n",
    "An algorithm may be highly sensitive to some of its features. The choose of good parameters may have a dominant effect on the algorithm performance. \n",
    "\n",
    "In this study, we use GridSearchCV to fine tune the algorithm. I start with default parameters and level it up and down. Based on the GridSearchCV function I will adjust the parameters again. For example, if the GridSearchCV chooses the smallest value for the parameter, I will add a smaller number in the search list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_features = pickle.load(open('train_features.pkl','br'))\n",
    "test_final_features = pickle.load(open('test_features.pkl','br'))\n",
    "train_labels = pickle.load(open('train_labels.pkl','br'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "xgb = xgboost.XGBRegressor(n_estimators=500, \n",
    "                           learning_rate=0.1, \n",
    "                           gamma=0,subsample=0.8,\n",
    "                           colsample_bytree=1,\n",
    "                           min_child_weight=1, \n",
    "                           max_depth=20,\n",
    "                           nthread=4,\n",
    "                           seed=1505)\n",
    "\n",
    "X = (train_final_features)\n",
    "Y = (train_labels)\n",
    "current_t = time.time()\n",
    "xgb.fit(X,Y)\n",
    "print(\"Modeling complete. Time elapsed: \" + str(int(time.time()-current_t)) + \"s\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating grid search\n",
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
     ]
    }
   ],
   "source": [
    "print(\"Initiating grid search\")\n",
    "X = (train_final_features)\n",
    "Y = (train_labels)\n",
    "current_t = time.time()\n",
    "xgb = xgboost.XGBRegressor(subsample=0.8,learning_rate=0.5)\n",
    "param_grid = { \"n_estimators\" : [300,500,800],\n",
    "                \"max_depth\" : [10,15,20], #17-11\n",
    "                \"min_child_weight\" : [1,11],\n",
    "                \"gamma\":[0,0.2,0.5],\n",
    "                   }\n",
    "CV_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid,verbose=1)\n",
    "X = (train_final_features)\n",
    "Y = (train_labels)\n",
    "CV_xgb.fit(X,Y)\n",
    "\n",
    "print(CV_xgb.best_params_,CV_xgb.best_score_) \n",
    "print(\"Modeling complete. Time elapsed: \" + str(int(time.time()-current_t)) + \"s\")\n",
    "xgb = CV_xgb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# vectorized error calc\\ndef rmsle(y, y0):\\n    assert len(y) == len(y0)\\n    return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))\\n    \\n# test\\ndef test_reg(reg, features, labels):\\n    features_train, features_test, labels_train, labels_test = train_test_split(                features, labels, test_size=0.8, random_state=0)\\n    ### fit the classifier using training set, and test on test set\\n    reg.fit(features_train, (labels_train))\\n    y_true = labels_test\\n    y_pred = (reg.predict(features_test))\\n    y_pred[y_pred<0]=0\\n    jag=rmsle(y_true,y_pred)\\n    print(jag)\\n\\n\\ntest_reg(xgb, train_final_features, train_labels)\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorized error calc\n",
    "def rmsle(y, y0):\n",
    "    assert len(y) == len(y0)\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))\n",
    "    \n",
    "# test\n",
    "def test_reg(reg, features, labels):\n",
    "    features_train, features_test, labels_train, labels_test = train_test_split(\\\n",
    "                features, labels, test_size=0.8, random_state=0)\n",
    "    ### fit the classifier using training set, and test on test set\n",
    "    reg.fit(features_train, (labels_train))\n",
    "    y_true = labels_test\n",
    "    y_pred = (reg.predict(features_test))\n",
    "    y_pred = np.exp(pred_label)\n",
    "    jag=rmsle(y_true,y_pred)\n",
    "    print(jag)\n",
    "\n",
    "\n",
    "test_reg(xgb, train_final_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling done!\n"
     ]
    }
   ],
   "source": [
    "outfile_name = 'submit.csv'\n",
    "\n",
    "pred_label = xgb.predict(test_final_features)\n",
    "pred_label = np.exp(pred_label)\n",
    "pred_label = pd.DataFrame(np.array(pred_label), columns = ['price'])\n",
    "pred_label.index.name = 'test_id'\n",
    "pred_label.to_csv(outfile_name, encoding='utf-8')\n",
    "print ('Modeling done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
